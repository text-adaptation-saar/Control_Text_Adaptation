# WikiLarge Dataset Preparation  

The **WikiLarge** dataset in tokenized format was obtained from [DRESS](https://github.com/XingxingZhang/dress). It was then de-tokenized using the [Detokenizer](https://github.com/sarubi/detokenizer.git).  

The original dataset contains **296,402 sentence pairs** in the training set.  

For our experiments, we only used the **WikiLarge training data**, as the **TurkCorpus** source and target have similar linguistic feature values and do not include sentence splitting.  
